{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (  DependencyGraph,   ProjectiveDependencyParser,    NonprojectiveDependencyParser)\n",
    "from nltk.parse.transitionparser import TransitionParser, Configuration, Transition\n",
    "from nltk.parse import ParserI, DependencyGraph, DependencyEvaluator\n",
    "import tempfile\n",
    "import os\n",
    "from numpy import array\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import svm\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from os import remove\n",
    "import tempfile\n",
    "import pickle\n",
    "from __future__ import print_function, unicode_literals\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from pprint import pformat\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "from six import string_types\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk.compat import python_2_unicode_compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDependencyGraph(object):\n",
    "    \"\"\"\n",
    "    A container for the nodes and labelled edges of a dependency structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tree_str=None, cell_extractor=None, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n",
    "\n",
    "        self.nodes = defaultdict(lambda:  {'address': None,\n",
    "                                           'word': None,\n",
    "                                           'lemma': None,\n",
    "                                           'ctag': None,\n",
    "                                           'tag': None,\n",
    "                                           'feats': None,\n",
    "                                           'head': None,\n",
    "                                           'deps': defaultdict(list),\n",
    "                                           'rel': None,\n",
    "                                            'misc':None,\n",
    "                                           })\n",
    "\n",
    "        self.nodes[0].update(\n",
    "            {\n",
    "                'ctag': 'TOP',\n",
    "                'tag': 'TOP',\n",
    "                'address': 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.root = None\n",
    "\n",
    "        if tree_str:\n",
    "            self._parse(\n",
    "                tree_str,\n",
    "                cell_extractor=cell_extractor,\n",
    "                zero_based=zero_based,\n",
    "                cell_separator=cell_separator,\n",
    "                top_relation_label=top_relation_label,\n",
    "            )\n",
    "\n",
    "    def remove_by_address(self, address):\n",
    "        \"\"\"\n",
    "        Removes the node with the given address.  References\n",
    "        to this node in others will still exist.\n",
    "        \"\"\"\n",
    "        del self.nodes[address]\n",
    "\n",
    "\n",
    "    def redirect_arcs(self, originals, redirect):\n",
    "        for node in self.nodes.values():\n",
    "            new_deps = []\n",
    "            for dep in node['deps']:\n",
    "                if dep in originals:\n",
    "                    new_deps.append(redirect)\n",
    "                else:\n",
    "                    new_deps.append(dep)\n",
    "            node['deps'] = new_deps\n",
    "\n",
    "\n",
    "    def add_arc(self, head_address, mod_address):\n",
    "        relation = self.nodes[mod_address]['rel']\n",
    "        self.nodes[head_address]['deps'].setdefault(relation, [])\n",
    "        self.nodes[head_address]['deps'][relation].append(mod_address)\n",
    "\n",
    "        #self.nodes[head_address]['deps'].append(mod_address)\n",
    "\n",
    "\n",
    "    def connect_graph(self):\n",
    "\n",
    "        for node1 in self.nodes.values():\n",
    "            for node2 in self.nodes.values():\n",
    "                if node1['address'] != node2['address'] and node2['rel'] != 'TOP':\n",
    "                    relation = node2['rel']\n",
    "                    node1['deps'].setdefault(relation, [])\n",
    "                    node1['deps'][relation].append(node2['address'])\n",
    "\n",
    "                    #node1['deps'].append(node2['address'])\n",
    "\n",
    "    def get_by_address(self, node_address):\n",
    "        \"\"\"Return the node with the given address.\"\"\"\n",
    "        return self.nodes[node_address]\n",
    "\n",
    "\n",
    "    def contains_address(self, node_address):\n",
    "        return node_address in self.nodes\n",
    "\n",
    "\n",
    "    def to_dot(self):\n",
    "        # Start the digraph specification\n",
    "        s = 'digraph G{\\n'\n",
    "        s += 'edge [dir=forward]\\n'\n",
    "        s += 'node [shape=plaintext]\\n'\n",
    "\n",
    "        # Draw the remaining nodes\n",
    "        for node in sorted(self.nodes.values(), key=lambda v: v['address']):\n",
    "            s += '\\n%s [label=\"%s (%s)\"]' % (node['address'], node['address'], node['word'])\n",
    "            for rel, deps in node['deps'].items():\n",
    "                for dep in deps:\n",
    "                    if rel is not None:\n",
    "                        s += '\\n%s -> %s [label=\"%s\"]' % (node['address'], dep, rel)\n",
    "                    else:\n",
    "                        s += '\\n%s -> %s ' % (node['address'], dep)\n",
    "        s += \"\\n}\"\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "    def _repr_svg_(self):\n",
    "        dot_string = self.to_dot()\n",
    "\n",
    "        try:\n",
    "            process = subprocess.Popen(\n",
    "                ['dot', '-Tsvg'],\n",
    "                stdin=subprocess.PIPE,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                universal_newlines=True,\n",
    "            )\n",
    "        except OSError:\n",
    "            raise Exception('Cannot find the dot binary from Graphviz package')\n",
    "        out, err = process.communicate(dot_string)\n",
    "        if err:\n",
    "            raise Exception(\n",
    "                'Cannot create svg representation by running dot from string: {}'\n",
    "                ''.format(dot_string))\n",
    "        return out\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(self.nodes)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<DependencyGraph with {0} nodes>\".format(len(self.nodes))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n",
    "        with open(filename) as infile:\n",
    "            return [\n",
    "                DependencyGraph(\n",
    "                    tree_str,\n",
    "                    zero_based=zero_based,\n",
    "                    cell_separator=cell_separator,\n",
    "                    top_relation_label=top_relation_label,\n",
    "                )\n",
    "                for tree_str in infile.read().split('\\n\\n')\n",
    "            ]\n",
    "\n",
    "\n",
    "    def left_children(self, node_index):\n",
    "        children = chain.from_iterable(self.nodes[node_index]['deps'].values())\n",
    "        index = self.nodes[node_index]['address']\n",
    "        return sum(1 for c in children if c < index)\n",
    "\n",
    "\n",
    "    def right_children(self, node_index):\n",
    "        children = chain.from_iterable(self.nodes[node_index]['deps'].values())\n",
    "        index = self.nodes[node_index]['address']\n",
    "        return sum(1 for c in children if c > index)\n",
    "\n",
    "\n",
    "    def add_node(self, node):\n",
    "        if not self.contains_address(node['address']):\n",
    "            self.nodes[node['address']].update(node)\n",
    "\n",
    "\n",
    "    def _parse(self, input_, cell_extractor=None, zero_based=False, cell_separator=None, top_relation_label='ROOT'):\n",
    "\n",
    "        def extract_3_cells(cells, index):\n",
    "            word, tag, head = cells\n",
    "            return index, word, word, tag, tag, '', head, ''\n",
    "\n",
    "        def extract_4_cells(cells, index):\n",
    "            word, tag, head, rel = cells\n",
    "            return index, word, word, tag, tag, '', head, rel\n",
    "\n",
    "        def extract_7_cells(cells, index):\n",
    "            line_index, word, lemma, tag, _, head, rel = cells\n",
    "            try:\n",
    "                index = int(line_index)\n",
    "            except ValueError:\n",
    "                # index can't be parsed as an integer, use default\n",
    "                pass\n",
    "            return index, word, lemma, tag, tag, '', head, rel\n",
    "\n",
    "        def extract_10_cells(cells, index):\n",
    "            line_index, word, lemma, ctag, tag, feats, head, rel, _, misc= cells\n",
    "            try:\n",
    "                index = int(line_index)\n",
    "            except ValueError:\n",
    "                # index can't be parsed as an integer, use default\n",
    "                pass\n",
    "            return index, word, lemma, ctag, tag, feats, head, rel,misc\n",
    "\n",
    "        extractors = {\n",
    "            3: extract_3_cells,\n",
    "            4: extract_4_cells,\n",
    "            7: extract_7_cells,\n",
    "            10: extract_10_cells,\n",
    "        }\n",
    "\n",
    "        if isinstance(input_, string_types):\n",
    "            input_ = (line for line in input_.split('\\n'))\n",
    "\n",
    "        lines = (l.rstrip() for l in input_)\n",
    "        lines = (l for l in lines if l)\n",
    "\n",
    "        cell_number = None\n",
    "        for index, line in enumerate(lines, start=1):\n",
    "            cells = line.split(cell_separator)\n",
    "            if cell_number is None:\n",
    "                cell_number = len(cells)\n",
    "            else:\n",
    "                assert cell_number == len(cells)\n",
    "\n",
    "            if cell_extractor is None:\n",
    "                try:\n",
    "                    cell_extractor = extractors[cell_number]\n",
    "                except KeyError:\n",
    "                    raise ValueError(\n",
    "                        'Number of tab-delimited fields ({0}) not supported by '\n",
    "                        'CoNLL(10) or Malt-Tab(4) format'.format(cell_number)\n",
    "                    )\n",
    "\n",
    "            try:\n",
    "                index, word, lemma, ctag, tag, feats, head, rel, misc = cell_extractor(cells, index)\n",
    "            except (TypeError, ValueError):\n",
    "                # cell_extractor doesn't take 2 arguments or doesn't return 8\n",
    "                # values; assume the cell_extractor is an older external\n",
    "                # extractor and doesn't accept or return an index.\n",
    "                word, lemma, ctag, tag, feats, head, rel = cell_extractor(cells)\n",
    "\n",
    "            if head == '_':\n",
    "                continue\n",
    "\n",
    "            head = int(head)\n",
    "            if zero_based:\n",
    "                head += 1\n",
    "\n",
    "            self.nodes[index].update(\n",
    "                {\n",
    "                    'address': index,\n",
    "                    'word': word,\n",
    "                    'lemma': lemma,\n",
    "                    'ctag': ctag,\n",
    "                    'tag': tag,\n",
    "                    'feats': feats,\n",
    "                    'head': head,\n",
    "                    'rel': rel,\n",
    "                    'misc':misc\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Make sure that the fake root node has labeled dependencies.\n",
    "            if (cell_number == 3) and (head == 0):\n",
    "                rel = top_relation_label\n",
    "            self.nodes[head]['deps'][rel].append(index)\n",
    "\n",
    "        if self.nodes[0]['deps'][top_relation_label]:\n",
    "            root_address = self.nodes[0]['deps'][top_relation_label][0]\n",
    "            self.root = self.nodes[root_address]\n",
    "            self.top_relation_label = top_relation_label\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"The graph doesn't contain a node \"\n",
    "                \"that depends on the root element.\"\n",
    "            )\n",
    "\n",
    "    def _word(self, node, filter=True):\n",
    "        w = node['word']\n",
    "        if filter:\n",
    "            if w != ',':\n",
    "                return w\n",
    "        return w\n",
    "\n",
    "    def _tree(self, i):\n",
    "        \"\"\" Turn dependency graphs into NLTK trees.\n",
    "\n",
    "        :param int i: index of a node\n",
    "        :return: either a word (if the indexed node is a leaf) or a ``Tree``.\n",
    "        \"\"\"\n",
    "        node = self.get_by_address(i)\n",
    "        word = node['word']\n",
    "        deps = sorted(chain.from_iterable(node['deps'].values()))\n",
    "\n",
    "        if deps:\n",
    "            return Tree(word, [self._tree(dep) for dep in deps])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def tree(self):\n",
    "        node = self.root\n",
    "\n",
    "        word = node['word']\n",
    "        deps = sorted(chain.from_iterable(node['deps'].values()))\n",
    "        return Tree(word, [self._tree(dep) for dep in deps])\n",
    "\n",
    "\n",
    "    def triples(self, node=None):\n",
    "        if not node:\n",
    "            node = self.root\n",
    "\n",
    "        head = (node['word'], node['ctag'])\n",
    "        for i in sorted(chain.from_iterable(node['deps'].values())):\n",
    "            dep = self.get_by_address(i)\n",
    "            yield (head, dep['rel'], (dep['word'], dep['ctag']))\n",
    "            for triple in self.triples(node=dep):\n",
    "                yield triple\n",
    "\n",
    "\n",
    "    def _hd(self, i):\n",
    "        try:\n",
    "            return self.nodes[i]['head']\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "    def _rel(self, i):\n",
    "        try:\n",
    "            return self.nodes[i]['rel']\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "    # what's the return type?  Boolean or list?\n",
    "    def contains_cycle(self):\n",
    "        distances = {}\n",
    "\n",
    "        for node in self.nodes.values():\n",
    "            for dep in node['deps']:\n",
    "                key = tuple([node['address'], dep])\n",
    "                distances[key] = 1\n",
    "\n",
    "        for _ in self.nodes:\n",
    "            new_entries = {}\n",
    "\n",
    "            for pair1 in distances:\n",
    "                for pair2 in distances:\n",
    "                    if pair1[1] == pair2[0]:\n",
    "                        key = tuple([pair1[0], pair2[1]])\n",
    "                        new_entries[key] = distances[pair1] + distances[pair2]\n",
    "\n",
    "            for pair in new_entries:\n",
    "                distances[pair] = new_entries[pair]\n",
    "                if pair[0] == pair[1]:\n",
    "                    path = self.get_cycle_path(self.get_by_address(pair[0]), pair[0])\n",
    "                    return path\n",
    "\n",
    "        return False  # return []?\n",
    "\n",
    "\n",
    "    def get_cycle_path(self, curr_node, goal_node_index):\n",
    "        for dep in curr_node['deps']:\n",
    "            if dep == goal_node_index:\n",
    "                return [curr_node['address']]\n",
    "        for dep in curr_node['deps']:\n",
    "            path = self.get_cycle_path(self.get_by_address(dep), goal_node_index)\n",
    "            if len(path) > 0:\n",
    "                path.insert(0, curr_node['address'])\n",
    "                return path\n",
    "        return []\n",
    "\n",
    "\n",
    "    def to_conll(self, style):\n",
    "\n",
    "\n",
    "        if style == 3:\n",
    "            template = '{word}\\t{tag}\\t{head}\\n'\n",
    "        elif style == 4:\n",
    "            template = '{word}\\t{tag}\\t{head}\\t{rel}\\n'\n",
    "        elif style == 10:\n",
    "            template = '{i}\\t{word}\\t{lemma}\\t{ctag}\\t{tag}\\t{feats}\\t{head}\\t{rel}\\t_\\t{misc}\\n'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Number of tab-delimited fields ({0}) not supported by '\n",
    "                'CoNLL(10) or Malt-Tab(4) format'.format(style)\n",
    "            )\n",
    "\n",
    "        return ''.join(template.format(i=i, **node) for i, node in sorted(self.nodes.items()) if node['tag'] != 'TOP')\n",
    "\n",
    "\n",
    "    def nx_graph(self):\n",
    "        \"\"\"Convert the data in a ``nodelist`` into a networkx labeled directed graph.\"\"\"\n",
    "        import networkx\n",
    "\n",
    "        nx_nodelist = list(range(1, len(self.nodes)))\n",
    "        nx_edgelist = [\n",
    "            (n, self._hd(n), self._rel(n))\n",
    "            for n in nx_nodelist if self._hd(n)\n",
    "        ]\n",
    "        self.nx_labels = {}\n",
    "        for n in nx_nodelist:\n",
    "            self.nx_labels[n] = self.nodes[n]['word']\n",
    "\n",
    "        g = networkx.MultiDiGraph()\n",
    "        g.add_nodes_from(nx_nodelist)\n",
    "        g.add_edges_from(nx_edgelist)\n",
    "\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(self, morph_status=False, misc_status=False):\n",
    "    '''\n",
    "       param morph_status: whether to take the morphological features in the feature set\n",
    "       param misc_statu: whether to take the additional features in the feature set\n",
    "    '''\n",
    "    result = []\n",
    "    # Todo : can come up with more complicated features set for better\n",
    "    # performance.\n",
    "    if len(self.stack) > 0:\n",
    "        # Stack 0\n",
    "        stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "        token = self._tokens[stack_idx0]\n",
    "        if self._check_informative(token['word'], True):\n",
    "            result.append('STK_0_FORM_' + token['word'])\n",
    "        if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "            result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "        if self._check_informative(token['tag']):\n",
    "            result.append('STK_0_POS_' + token['tag'])\n",
    "        if(morph_status):\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "        \n",
    "        if(misc_status):\n",
    "            if 'misc' in token and self._check_informative(token['misc']):\n",
    "                miscs = token['misc'].split(\"|\")\n",
    "                for misc in miscs:\n",
    "                    result.append('STK_0_MISC_' + misc)\n",
    "        # Stack 1\n",
    "        if len(self.stack) > 1:\n",
    "            stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "            token = self._tokens[stack_idx1]\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "        # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = ''\n",
    "        dep_right_most = ''\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == stack_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append('STK_0_LDEP_' + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "    # Check Buffered 0\n",
    "    if len(self.buffer) > 0:\n",
    "        # Buffer 0\n",
    "        buffer_idx0 = self.buffer[0]\n",
    "        token = self._tokens[buffer_idx0]\n",
    "        if self._check_informative(token['word'], True):\n",
    "            result.append('BUF_0_FORM_' + token['word'])\n",
    "        if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "            result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "        if self._check_informative(token['tag']):\n",
    "            result.append('BUF_0_POS_' + token['tag'])\n",
    "        if(morph_status):\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "        if (misc_status):\n",
    "            if 'misc' in token and self._check_informative(token['misc']):\n",
    "                miscs = token['misc'].split(\"|\")\n",
    "                for misc in miscs:\n",
    "                    result.append('BUF_0_MISC_' + misc) \n",
    "        # Buffer 1\n",
    "        if len(self.buffer) > 1:\n",
    "            buffer_idx1 = self.buffer[1]\n",
    "            token = self._tokens[buffer_idx1]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_1_FORM_' + token['word'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_1_POS_' + token['tag'])\n",
    "        if len(self.buffer) > 2:\n",
    "            buffer_idx2 = self.buffer[2]\n",
    "            token = self._tokens[buffer_idx2]\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_2_POS_' + token['tag'])\n",
    "        if len(self.buffer) > 3:\n",
    "            buffer_idx3 = self.buffer[3]\n",
    "            token = self._tokens[buffer_idx3]\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_3_POS_' + token['tag'])\n",
    "                # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = ''\n",
    "        dep_right_most = ''\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == buffer_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "    return result\n",
    "\n",
    "Configuration.extract_features = extractFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(parser, depgraphs, modelfile, verbose=True):\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train',dir=tempfile.gettempdir(), delete=False)\n",
    "\n",
    "        if parser._algorithm == parser.ARC_STANDARD:\n",
    "            parser._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            parser._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, 'wb'))\n",
    "    finally:\n",
    "        remove(input_file.name)\n",
    "        return modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UD_Hindi/hi-ud-train.conllu', 'r') as f:\n",
    "    graphs = [myDependencyGraph(entry, top_relation_label='root') for entry in f.read().split('\\n\\n') if entry]\n",
    "with open('UD_Hindi/hi-ud-test.conllu', 'r') as f:\n",
    "    graph_test = [myDependencyGraph(entry, top_relation_label='root') for entry in f.read().split('\\n\\n') if entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_str_list=['arc-eager','arc-standard']\n",
    "model_list=[svm.SVC(kernel='poly',degree=2, coef0=0,gamma=0.2,C=0.5,verbose=True,probability=True),MLPClassifier(solver='lbfgs', alpha=1e-3,hidden_layer_sizes=100, random_state=1, verbose=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parser_string in parser_str_list:\n",
    "    for model in model_list:\n",
    "        parser=TransitionParser('arc-standard')\n",
    "        train_model(parser,model,graphs,'temp.'+parser_string+'.model',True)\n",
    "        result = parser.parse(graph_test, 'temp.'+parser_string+'.model')\n",
    "        evaluator = DependencyEvaluator(result,graph_test)\n",
    "        print('Peformance for ',parser_string,' parser and model ',str(model),' :')\n",
    "        print(evaluator.eval())\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8488284202569917, 0.764928193499622)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
