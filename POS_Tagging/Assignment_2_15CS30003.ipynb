{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "import copy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from nltk import bigrams, ngrams, trigrams \n",
    "from nltk.probability import FreqDist as FreqDist  \n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from collections import Counter\n",
    "from string import digits\n",
    "from scipy import stats\n",
    "import operator\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank,brown\n",
    "\n",
    "corpus = brown.tagged_sents(tagset='universal')[:-100] \n",
    "\n",
    "tag_dict={}\n",
    "word_dict={}\n",
    "\n",
    "for sent in corpus:\n",
    "    for elem in sent:\n",
    "        w = elem[0]\n",
    "        tag= elem[1]\n",
    "\n",
    "        if w not in word_dict:\n",
    "            word_dict[w]=0\n",
    "\n",
    "        if tag not in tag_dict:\n",
    "            tag_dict[tag]=0\n",
    "\n",
    "        word_dict[w]+=1\n",
    "        tag_dict[tag]+=1\n",
    "unigram_prob=dict()\n",
    "V=len(word_dict)\n",
    "for word in word_dict.keys():\n",
    "    unigram_prob[word]=word_dict[word]/V\n",
    "test_data= brown.tagged_sents(tagset='universal')[-100:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the start, emission and transition probability matrices for the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start={}\n",
    "transition={}\n",
    "emission={}\n",
    "context={}\n",
    "for line in corpus:\n",
    "    previous='<s>'\n",
    "    for word,tag in line:\n",
    "        if(previous=='<s>'):                           # First tag of the sentence\n",
    "            if tag not in context:\n",
    "                context[tag]=0\n",
    "            context[tag]+=1\n",
    "            if tag not in start:\n",
    "                start[tag]=0.0\n",
    "            start[tag]+=1\n",
    "            previous=tag\n",
    "        else:                                          # Rest of the word-tag pairs\n",
    "            if tag not in context:\n",
    "                context[tag]=0\n",
    "            context[tag]+=1\n",
    "            if previous not in transition:\n",
    "                transition[previous]={}\n",
    "            if tag not in transition[previous]:\n",
    "                transition[previous][tag]=0\n",
    "            transition[previous][tag]+=1\n",
    "            previous=tag\n",
    "        if tag not in emission:\n",
    "            emission[tag]={}\n",
    "        if word not in emission[tag]:\n",
    "            emission[tag][word]=0\n",
    "        emission[tag][word]+=1\n",
    "    if '</s>' not in transition[previous]:\n",
    "        transition[previous]['</s>']=0\n",
    "    transition[previous]['</s>']+=1\n",
    "emission_raw=copy.deepcopy(emission)\n",
    "for tag in transition:                                 # Normalizing the counts into probability scores\n",
    "    total_count=float(sum(transition[tag].values())) \n",
    "    for next_tag in transition[tag]:\n",
    "        transition[tag][next_tag]/=(total_count)\n",
    "for tag in start:                                      # Normalizing the counts into probability scores\n",
    "    start[tag]/=float(len(corpus))\n",
    "\n",
    "for tag in emission:                                   # Normalizing the counts into probability scores\n",
    "    total_count=float(sum(emission[tag].values())) \n",
    "    for word in emission[tag]:\n",
    "        emission[tag][word]/=(total_count)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Taggging with HMM Model and Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_emission(tag, word,delta):                          # Emission probabilities with additive smoothing\n",
    "    count_pair=emission_raw[tag].get(word,0)\n",
    "    prob=(count_pair+delta)/(context[tag]+delta*V)\n",
    "    return prob\n",
    "\n",
    "def log(x):\n",
    "    if (x==0 ):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return np.log(x)\n",
    "    \n",
    "def pos_tagger(sentence,delta):\n",
    "    viterbi_scores=dict()\n",
    "    backpointer=dict()\n",
    "    N=len(context.keys())                                         # Number of states except accept state '</s>'\n",
    "    T=len(sentence)\n",
    "    words=['dummy']\n",
    "    words.extend(sentence)\n",
    "    for s in context.keys():                                      # Initialization Step\n",
    "        viterbi_scores[s,1]=log((start.get(s,0))*smoothed_emission(s,words[1],delta))\n",
    "        backpointer[s,1]=0\n",
    "    for t in range(2,T+1):                                        # Recursion Step\n",
    "        for s in context.keys():\n",
    "            max_prob=-np.inf\n",
    "            for s1 in context.keys():\n",
    "                prob=viterbi_scores[s1,t-1]\n",
    "                prob+=log(transition[s1].get(s,0))\n",
    "                prob+=log(smoothed_emission(s,words[t],delta))\n",
    "                if(prob>=max_prob):\n",
    "                    viterbi_scores[s,t]=prob\n",
    "                    backpointer[s,t]=s1\n",
    "                    max_prob=prob\n",
    "    max_prob=-np.inf\n",
    "    for s in context.keys():                                      # Termination Step                                 \n",
    "        prob=(viterbi_scores[s,T])+log(transition[s].get('</s>',0))\n",
    "        if prob>=max_prob:\n",
    "            max_prob=prob\n",
    "            viterbi_scores['</s>',T+1]=prob\n",
    "            backpointer['</s>',T+1]=s      \n",
    "    tag_sequence=list()\n",
    "    tag='</s>'\n",
    "    tag_sequence.append(tag)\n",
    "    for t in range(T+1,0,-1):                                     # Trace the backpointers to get the tag sequence\n",
    "        tag=backpointer[tag,t]\n",
    "        tag_sequence.append(tag)\n",
    "    tag_sequence.reverse()\n",
    "    tag_sequence=tag_sequence[1:len(tag_sequence)-1]\n",
    "    return tag_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(predicted_tags, test_tags):\n",
    "    num_sentences=len(predicted_tags)\n",
    "    scores=list()\n",
    "    total_count=0\n",
    "    num_tags=0\n",
    "    for i in range(num_sentences):\n",
    "        count=0\n",
    "        predicted=predicted_tags[i]\n",
    "        test=test_tags[i]\n",
    "        len_sentence=len(test)\n",
    "        for j in range(len_sentence):\n",
    "            if(predicted[j]==test[j]):\n",
    "                count+=1\n",
    "                total_count+=1\n",
    "            #else:\n",
    "                #print(predicted[j],test[j])\n",
    "        #if(count/len_sentence<0.7):\n",
    "            #print(test_sentences[i])\n",
    "            #print(predicted)\n",
    "            #print(test)\n",
    "            #print(\"\\n\")\n",
    "        scores.append(count/len_sentence)\n",
    "        num_tags+=len_sentence\n",
    "    print(total_count/num_tags)\n",
    "    return np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences=[[word[0] for word in sentence] for sentence in test_data]\n",
    "test_tags=[[word[1] for word in sentence] for sentence in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags=[pos_tagger(sentence,0.001) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9227215455690886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9103542356640717"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_accuracy(predicted_tags,test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lis=['S.', 'J.', 'Perelman']\n",
    "pos_tagger(lis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
